{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install ssqueezepy\n!pip install timm\n!pip install pytorch-lightning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-16T08:11:19.959525Z","iopub.execute_input":"2022-05-16T08:11:19.960224Z","iopub.status.idle":"2022-05-16T08:11:49.676142Z","shell.execute_reply.started":"2022-05-16T08:11:19.960125Z","shell.execute_reply":"2022-05-16T08:11:49.674917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_path='../input/eeg-data-distance-learning-environment'","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:12:38.720944Z","iopub.execute_input":"2022-05-16T08:12:38.721776Z","iopub.status.idle":"2022-05-16T08:12:38.726422Z","shell.execute_reply.started":"2022-05-16T08:12:38.721721Z","shell.execute_reply":"2022-05-16T08:12:38.725477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\ndf=pd.read_csv(os.path.join(main_path,'EEG_data.csv'))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:12:51.51902Z","iopub.execute_input":"2022-05-16T08:12:51.519317Z","iopub.status.idle":"2022-05-16T08:12:52.978393Z","shell.execute_reply.started":"2022-05-16T08:12:51.519268Z","shell.execute_reply":"2022-05-16T08:12:52.977704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we need first 16 channels to get raw data 14 channels\n\ncols_remove=df.columns.tolist()[16:-1]\ndf=df.loc[:, ~df.columns.isin(cols_remove)]\ndf.columns = df.columns.str.strip('EEG.')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:12:58.804973Z","iopub.execute_input":"2022-05-16T08:12:58.805291Z","iopub.status.idle":"2022-05-16T08:12:58.836616Z","shell.execute_reply.started":"2022-05-16T08:12:58.805242Z","shell.execute_reply":"2022-05-16T08:12:58.835548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['subject_understood'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:12:59.91882Z","iopub.execute_input":"2022-05-16T08:12:59.91948Z","iopub.status.idle":"2022-05-16T08:12:59.930952Z","shell.execute_reply.started":"2022-05-16T08:12:59.919442Z","shell.execute_reply":"2022-05-16T08:12:59.93018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now i need to reshape the data, into subjects,trials,channels,length\n#for that first i will create groups based on subjects\ngroups=df.groupby(['subject_id','video_id'])\ngrp_keys=list(groups.groups.keys())\nprint(grp_keys)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:13:02.184624Z","iopub.execute_input":"2022-05-16T08:13:02.184891Z","iopub.status.idle":"2022-05-16T08:13:02.464364Z","shell.execute_reply.started":"2022-05-16T08:13:02.184864Z","shell.execute_reply":"2022-05-16T08:13:02.463598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grpno=grp_keys[0]\ngrp1=groups.get_group(grpno).drop(['subject_id','video_id'],axis=1)\nlabel=grp1['subject_understood']\nsubject_id=grpno[0]\ngrp1=grp1.drop('subject_understood',axis=1)\ngrp1.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:14:27.785874Z","iopub.execute_input":"2022-05-16T08:14:27.786642Z","iopub.status.idle":"2022-05-16T08:14:27.816566Z","shell.execute_reply.started":"2022-05-16T08:14:27.786603Z","shell.execute_reply":"2022-05-16T08:14:27.815822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mne\ndef convertDF2MNE(sub):\n    info = mne.create_info(list(sub.columns), ch_types=['eeg'] * len(sub.columns), sfreq=128)\n    info.set_montage('standard_1020')\n    data=mne.io.RawArray(sub.T, info)\n    data.set_eeg_reference()\n    #data.filter(l_freq=1,h_freq=30)\n    epochs=mne.make_fixed_length_epochs(data,duration=3,overlap=2)\n    return epochs.get_data()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:17:18.883824Z","iopub.execute_input":"2022-05-16T08:17:18.884085Z","iopub.status.idle":"2022-05-16T08:17:20.147933Z","shell.execute_reply.started":"2022-05-16T08:17:18.884057Z","shell.execute_reply":"2022-05-16T08:17:20.147208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=convertDF2MNE(grp1)\ntest.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:17:27.001782Z","iopub.execute_input":"2022-05-16T08:17:27.002095Z","iopub.status.idle":"2022-05-16T08:17:27.691287Z","shell.execute_reply.started":"2022-05-16T08:17:27.002056Z","shell.execute_reply":"2022-05-16T08:17:27.690352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"128*3","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:17:42.632414Z","iopub.execute_input":"2022-05-16T08:17:42.63309Z","iopub.status.idle":"2022-05-16T08:17:42.637848Z","shell.execute_reply.started":"2022-05-16T08:17:42.63305Z","shell.execute_reply":"2022-05-16T08:17:42.637137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir scaleogram","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:34:00.692099Z","iopub.execute_input":"2022-05-16T08:34:00.692667Z","iopub.status.idle":"2022-05-16T08:34:01.402097Z","shell.execute_reply.started":"2022-05-16T08:34:00.692628Z","shell.execute_reply":"2022-05-16T08:34:01.401135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimport scipy.io\nimport torch.nn as nn\nimport torch\nimport numpy as np\nimport mne\nfrom ssqueezepy import cwt\nfrom ssqueezepy.visuals import plot, imshow\nimport os\nimport re\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:34:25.652106Z","iopub.execute_input":"2022-05-16T08:34:25.652434Z","iopub.status.idle":"2022-05-16T08:34:29.424392Z","shell.execute_reply.started":"2022-05-16T08:34:25.652397Z","shell.execute_reply":"2022-05-16T08:34:29.423642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:35:43.458182Z","iopub.execute_input":"2022-05-16T08:35:43.458484Z","iopub.status.idle":"2022-05-16T08:35:43.464306Z","shell.execute_reply.started":"2022-05-16T08:35:43.458456Z","shell.execute_reply":"2022-05-16T08:35:43.463531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Wx, scales = cwt(test[0], 'morlet')\nWx.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:36:34.126933Z","iopub.execute_input":"2022-05-16T08:36:34.127488Z","iopub.status.idle":"2022-05-16T08:36:34.205525Z","shell.execute_reply.started":"2022-05-16T08:36:34.127446Z","shell.execute_reply":"2022-05-16T08:36:34.204741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imshow(Wx[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:37:16.656726Z","iopub.execute_input":"2022-05-16T08:37:16.656991Z","iopub.status.idle":"2022-05-16T08:37:16.853774Z","shell.execute_reply.started":"2022-05-16T08:37:16.656964Z","shell.execute_reply":"2022-05-16T08:37:16.853129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\ngrpnos,labels,paths=[],[],[]\nfor i,grpno in enumerate(grp_keys):\n    grp=groups.get_group(grpno).drop(['subject_id','video_id'],axis=1)\n    label=int(grp['subject_understood'].unique())\n    subject_id=grpno[0]\n    grp=grp.drop('subject_understood',axis=1)\n    data=convertDF2MNE(grp)#(trials, channels, length)\n    for c,x in enumerate(data):#loop trials\n        Wx, scales = cwt(x, 'morlet')\n        Wx=np.abs(Wx)\n        path=os.path.join('./scaleogram',f'subvideo_{grpno}/',)\n        os.makedirs(path,exist_ok=True)\n        path=path+f'trial_{c}.npy'\n        np.save(path,Wx)\n        \n        grpnos.append(i)\n        labels.append(label)\n        paths.append(path)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:40:02.693826Z","iopub.execute_input":"2022-05-16T08:40:02.694126Z","iopub.status.idle":"2022-05-16T08:40:39.452262Z","shell.execute_reply.started":"2022-05-16T08:40:02.694094Z","shell.execute_reply":"2022-05-16T08:40:39.451397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Wx, scales = cwt(x, 'morlet')\nimshow(Wx[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:41:13.391668Z","iopub.execute_input":"2022-05-16T08:41:13.392143Z","iopub.status.idle":"2022-05-16T08:41:13.645695Z","shell.execute_reply.started":"2022-05-16T08:41:13.392104Z","shell.execute_reply":"2022-05-16T08:41:13.645029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scale=pd.DataFrame(zip(paths,labels,grpnos),columns=['path','label','group'])\ndf_scale.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:41:18.806165Z","iopub.execute_input":"2022-05-16T08:41:18.806745Z","iopub.status.idle":"2022-05-16T08:41:18.817645Z","shell.execute_reply.started":"2022-05-16T08:41:18.806708Z","shell.execute_reply":"2022-05-16T08:41:18.816854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom pytorch_lightning import seed_everything, LightningModule, Trainer\nfrom sklearn.utils import class_weight\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data.dataloader import DataLoader\nfrom pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor\nfrom torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau,CosineAnnealingWarmRestarts,OneCycleLR,CosineAnnealingLR\nimport torchvision\nfrom sklearn.metrics import classification_report,f1_score,accuracy_score,roc_curve,auc,roc_auc_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom PIL import Image\nimport cv2\nfrom torch.utils.data import DataLoader, Dataset,ConcatDataset\nimport torchmetrics\nimport timm\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:44:55.856487Z","iopub.execute_input":"2022-05-16T08:44:55.857217Z","iopub.status.idle":"2022-05-16T08:45:02.271095Z","shell.execute_reply.started":"2022-05-16T08:44:55.857178Z","shell.execute_reply":"2022-05-16T08:45:02.270334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read data from folders\nclass DataReader(Dataset):\n    def __init__(self, dataset,aug=None):\n        self.dataset = dataset\n        self.aug=aug\n    def __getitem__(self, index):\n        x=self.dataset.path[index]\n        y=self.dataset.label[index]\n        x=np.load(x)\n        if self.aug:\n          if random.uniform(0, 1)>0.5:\n            x=np.flip(x,-1)\n          if random.uniform(0, 1)>0.5:\n            x=np.flip(x,-2)\n          # if random.uniform(0, 1)>0.5:\n          #   c=np.arange(14)\n          #   np.random.shuffle(c)\n          #  x=x[c,:,:]\n        x=(x - np.min(x)) / (np.max(x) - np.min(x))\n       \n        return x, y\n    \n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:46:15.725971Z","iopub.execute_input":"2022-05-16T08:46:15.726261Z","iopub.status.idle":"2022-05-16T08:46:15.733754Z","shell.execute_reply.started":"2022-05-16T08:46:15.726229Z","shell.execute_reply":"2022-05-16T08:46:15.733032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader=DataLoader(DataReader(df_scale,True), batch_size =8)\ntest_batch=next(iter(test_loader))\ntest_batch[0].shape ,test_batch[1].shape ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:46:40.046306Z","iopub.execute_input":"2022-05-16T08:46:40.046569Z","iopub.status.idle":"2022-05-16T08:46:40.148911Z","shell.execute_reply.started":"2022-05-16T08:46:40.046542Z","shell.execute_reply":"2022-05-16T08:46:40.14822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\nclass OurModel(LightningModule):\n    def __init__(self,train_split,val_split):\n        super(OurModel,self).__init__()\n        #architecute\n        #lambda resnet\n        \n        self.train_split=train_split\n        self.val_split=val_split\n        #########TIMM#################\n        model_name='resnest26d'\n        self.model =  timm.create_model(model_name,pretrained=True)\n        self.model.conv1[0]=nn.Conv2d(14, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n       \n\n        self.fc1=nn.Linear(1000,500)\n        self.relu=nn.ReLU()\n        self.fc2= nn.Linear(500,250)\n        self.fc3= nn.Linear(250,1)\n        self.drp=nn.Dropout(0.25)\n        #parameters\n        self.lr=1e-3\n        self.batch_size=16\n        self.numworker=2\n        self.criterion=nn.BCEWithLogitsLoss()\n        self.metrics=torchmetrics.Accuracy()\n\n        self.trainloss,self.valloss=[],[]\n        self.trainacc,self.valacc=[],[]\n        \n        self.sub_pred=0\n    def forward(self,x):\n        x= self.model(x)\n        x=self.fc1(x)\n        x=self.relu(x)\n        x=self.drp(x)\n        x=self.fc2(x)\n        x=self.relu(x)\n        x=self.drp(x)\n        x=self.fc3(x)\n        return x\n\n    def configure_optimizers(self):\n        opt=torch.optim.AdamW(params=self.parameters(),lr=self.lr )\n        return opt\n        \n    def train_dataloader(self):\n        return DataLoader(DataReader(self.train_split,False), batch_size = self.batch_size, \n                          num_workers=self.numworker,pin_memory=True,shuffle=True)\n\n    def training_step(self,batch,batch_idx):\n        image,label=batch\n        pred = self(image)\n        loss=self.criterion(pred.flatten(),label.float()) #calculate loss\n        acc=self.metrics(pred.flatten(),label)#calculate accuracy\n        return {'loss':loss,'acc':acc}\n\n    def training_epoch_end(self, outputs):\n        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\n        acc=torch.stack([x[\"acc\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\n        self.trainloss.append(loss)\n        self.trainacc.append(acc)\n        #print('training acc',acc)\n        self.log('train_loss', loss)\n        \n    def val_dataloader(self):\n        ds=DataLoader(DataReader(self.val_split), batch_size = self.batch_size,\n                      num_workers=self.numworker,pin_memory=True, shuffle=False)\n        return ds\n\n    def validation_step(self,batch,batch_idx):\n        image,label=batch\n        pred = self(image)\n        loss=self.criterion(pred.flatten(),label.float()) #calculate loss\n        acc=self.metrics(pred.flatten(),label)#calculate accuracy\n        return {'loss':loss,'acc':acc}\n\n    def validation_epoch_end(self, outputs):\n        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\n        acc=torch.stack([x[\"acc\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\n        self.valloss.append(loss)\n        self.valacc.append(acc)\n        #print('validation acc',self.current_epoch,acc)\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n      \n    def test_dataloader(self):\n        ds=DataLoader(DataReader(self.val_split), batch_size = self.batch_size,\n                      num_workers=self.numworker,pin_memory=True, shuffle=False)\n        return ds\n    def test_step(self,batch,batch_idx):\n        image,label=batch\n        pred = self(image)\n        \n        return {'label':label,'pred':pred}\n\n    def test_epoch_end(self, outputs):\n\n        label=torch.cat([x[\"label\"] for x in outputs])\n        pred=torch.cat([x[\"pred\"] for x in outputs])\n        acc=self.metrics(pred.flatten(),label)\n        pred=pred.detach().cpu().numpy().ravel()\n        label=label.detach().cpu().numpy().ravel()\n        print('sklearn auc',roc_auc_score(label,pred))\n        pred=np.where(pred>0.5,1,0).astype(int)\n        print('torch acc',acc)\n        print(classification_report(label,pred))\n        print('sklearn',accuracy_score(label,pred))\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:50:29.447722Z","iopub.execute_input":"2022-05-16T08:50:29.448004Z","iopub.status.idle":"2022-05-16T08:50:29.472917Z","shell.execute_reply.started":"2022-05-16T08:50:29.447974Z","shell.execute_reply":"2022-05-16T08:50:29.472213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold,LeaveOneGroupOut,StratifiedGroupKFold\ngkf=StratifiedGroupKFold(5)\nresult=[]\nvalacc=[]\nfor train_index, val_index in gkf.split(df_scale.path,df_scale.label,  groups=df_scale.group):\n    train_df=df_scale.iloc[train_index].reset_index(drop=True)\n    val_df=df_scale.iloc[val_index].reset_index(drop=True)\n\n\n    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n    gpu=-1 if torch.cuda.is_available() else 0\n    gpup=16 if torch.cuda.is_available() else 32\n    model=OurModel(train_df,val_df)\n    trainer = Trainer(max_epochs=20, auto_lr_find=True, auto_scale_batch_size=True,\n                        deterministic=True,\n                        gpus=gpu,precision=gpup,\n                        accumulate_grad_batches=2,\n                        enable_progress_bar = True,\n                        num_sanity_val_steps=0,\n                        callbacks=[lr_monitor],\n   \n                        )\n    trainer.fit(model)\n    res=trainer.validate(model)\n    result.append(res)\n    valacc.append(model.valacc)\n    trainer.test(model)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T08:50:48.023825Z","iopub.execute_input":"2022-05-16T08:50:48.024059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.batch_size","metadata":{"execution":{"iopub.status.busy":"2022-05-05T00:51:07.10088Z","iopub.status.idle":"2022-05-05T00:51:07.101493Z","shell.execute_reply.started":"2022-05-05T00:51:07.101235Z","shell.execute_reply":"2022-05-05T00:51:07.101262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(model.trainacc,label='train')\nplt.plot(model.valacc,label='val')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T00:43:40.498498Z","iopub.status.idle":"2022-05-05T00:43:40.4991Z","shell.execute_reply.started":"2022-05-05T00:43:40.498857Z","shell.execute_reply":"2022-05-05T00:43:40.498885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df.label.unique(),val_df.group.unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T00:43:40.500239Z","iopub.status.idle":"2022-05-05T00:43:40.500838Z","shell.execute_reply.started":"2022-05-05T00:43:40.500564Z","shell.execute_reply":"2022-05-05T00:43:40.5006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}